{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdfJ53H2zo4c"
   },
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTK3FuTTZxxm"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 12841,
     "status": "ok",
     "timestamp": 1752761704872,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "kkB-xJWBPsqj"
   },
   "outputs": [],
   "source": [
    "#Import of relevant libraries, classes or methods (in order of apperance)\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import joblib\n",
    "import shap\n",
    "import json\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current notebook directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Go one level up and build the path to the data folder\n",
    "\n",
    "data_path_df = os.path.join(current_dir, \"..\", \"data\", \n",
    "                         \"kaggle_survey_2020_responses.csv\")\n",
    "\n",
    "data_path_encoder_assignment = os.path.join(current_dir, \"..\", \"data\", \n",
    "                         \"encoder_assignment.csv\")\n",
    "\n",
    "data_path_unique_with_rank = os.path.join(current_dir, \"..\", \"data\", \n",
    "                         \"unique_with_rank.csv\")\n",
    "\n",
    "\n",
    "# Load the CSV files and instantiate DataFrames\n",
    "\n",
    "df                 = pd.read_csv(data_path_df,\n",
    "                                 dtype = 'str')\n",
    "\n",
    "encoder_assignment = pd.read_csv(data_path_encoder_assignment,\n",
    "                                 sep = ';')\n",
    "\n",
    "\n",
    "unique_with_rank = pd.read_csv(data_path_unique_with_rank, \n",
    "                               dtype = 'str',\n",
    "                               sep = ';',\n",
    "                               na_values=['nan', '', 'NaN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xANjAbYFaJ3j"
   },
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1752761887337,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "N11d-rFuSwQW"
   },
   "outputs": [],
   "source": [
    "# Merge the question numbers with the question text for better readibility\n",
    " # We will shorten the column names later.\n",
    "\n",
    "col_index = df.columns.tolist()\n",
    "# print(col_index)\n",
    "\n",
    "questions = df.iloc[0,:].tolist()\n",
    "# print(questions)\n",
    "\n",
    "# Generate the composed index in a new list. Use a list comprehenshion with built in zip function:\n",
    "col_index_new = [f\"{col}_{val}\" for col, val in zip(col_index, questions)]\n",
    "\n",
    "#  print(col_index_new)\n",
    "\n",
    "# Step 2: Assign the new index to the DataFrame\n",
    "df.columns = col_index_new\n",
    "\n",
    "# Step 3: Delete the first row (no longer used)\n",
    "df = df.iloc[1:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2958,
     "status": "ok",
     "timestamp": 1752761890298,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "JASao2C0qH-m"
   },
   "outputs": [],
   "source": [
    "# Within the 'Selected Choice'-columns, it is better to transform the values\n",
    "# into Booleans, because they are better to count and better to modell.\n",
    "# All others are strings. An NaN is interpreted as \"z_not selected\"\n",
    "\n",
    "mark_bool = '- Selected Choice -'                     # If the questions have \"- Seleceted choice - \", the answer ist yes or no (Boolean)\n",
    "\n",
    "\n",
    "for col in df.columns:                                # NaNs are interpreted as False\n",
    "    if mark_bool in col:\n",
    "        df[col] = df[col].apply(lambda x:\n",
    "                                False if pd.isna(x)\n",
    "                                else True)            # If it's not a NaN, the answer is given. We replace it with a True\n",
    "    else:\n",
    "        df[col] = df[col].fillna('z_Not selected')     # NaNs are interpreted as \"z_Not selected\". z as a prefix to secure that it is always on the right hand side of the plot.\n",
    "        df[col] = df[col].astype(str)                  # The whole columns is transformed into a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1752761890303,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "_Jm8yi_KXRLP"
   },
   "outputs": [],
   "source": [
    "# We now choose the questions of boolean type and assign them do a DataFram df_bool::\n",
    "\n",
    "# Questions with boolean character:\n",
    "\n",
    "col_bool = ['^Q7', '^Q9', '^Q14', '^Q16', '^Q17', '^Q18', '^Q19',\n",
    "            '^Q26_A', '^Q27_A', '^Q28_A', '^Q29_A', '^Q31_A', '^Q33_A', '^Q34_A', '^Q35_A', '^Q37']\n",
    "\n",
    "# Filter rules\n",
    "regex_pattern_bool = '|'.join(col_bool)\n",
    "\n",
    "# Filter all Boolean columns\n",
    "df_bool = df.filter(regex=regex_pattern_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 298,
     "status": "ok",
     "timestamp": 1752761890603,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "Y0zAtZ4_Xrhf",
    "outputId": "55b9e739-1f2c-4d0d-d730-5b43e184729e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\178850794.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\178850794.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\178850794.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\178850794.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\178850794.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\178850794.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\178850794.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\178850794.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\178850794.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\178850794.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\178850794.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\178850794.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\178850794.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\178850794.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\178850794.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\178850794.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\178850794.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\178850794.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\178850794.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\178850794.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\178850794.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\178850794.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\178850794.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\178850794.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\178850794.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\178850794.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\178850794.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\178850794.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\178850794.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\178850794.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\178850794.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\178850794.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n"
     ]
    }
   ],
   "source": [
    "# Define a pattern to sum up the relevant columns\n",
    "\n",
    "# Mapping Regex → Columns for summing up\n",
    "sum_columns = {\n",
    "    '^Q7':    'Q7_No. of Regular used programming languages?',\n",
    "    '^Q9':    'Q9_No. of Specialized IDE?',\n",
    "    '^Q14':   'Q14_No. of DataViz Libs or Tools?',\n",
    "    '^Q16':   'Q16_No. of ML Framworks?',\n",
    "    '^Q17':   'Q17_No. of ML algorithms?',\n",
    "    '^Q18':   'Q18_No. of Computer Vsion methods?',\n",
    "    '^Q19':   'Q19_No. of NLP methods?',\n",
    "    '^Q26_A': 'Q26_A No. of Current Cloud platforms?',\n",
    "    '^Q27_A': 'Q27_A No. of Current Cloud Products?',\n",
    "    '^Q28_A': 'Q28_A No. of Current ML products?',\n",
    "    '^Q29_A': 'Q29_A No. of Big Data Tools?',\n",
    "    '^Q31_A': 'Q31_A No. of BI Tools used?',\n",
    "    '^Q33_A': 'Q33_A No. of Automted ML Tools?',\n",
    "    '^Q34_A': 'Q34_A No. of Auto ML Tools?',\n",
    "    '^Q35_A': 'Q35_A No. of ML Experiment Management?',\n",
    "    '^Q37':   'Q37_No. of Learning Platforms?'\n",
    "\n",
    "}\n",
    "\n",
    "for regex, new_col in sum_columns.items():\n",
    "    matching_cols = df.filter(regex=regex).columns                                         # Select all columns that match\n",
    "    valid_cols = [col for col in matching_cols if \"- Selected Choice - None\" not in col]   # Exclude columns with \"none\"\n",
    "    df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1752761890607,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "SR_jNhq25MG9"
   },
   "outputs": [],
   "source": [
    "# We now choose the questions of string type (multiple choice) and assign them do a DataFram df_string::\n",
    "\n",
    "col_string = ['Q1_What is your age (# years)?',\n",
    "              'Q4_What is the highest level of formal education that you have attained or plan to attain within the next 2 years?',\n",
    "              'Q5_Select the title most similar to your current role (or most recent title if retired): - Selected Choice',\n",
    "              'Q6_For how many years have you been writing code and/or programming?',\n",
    "              'Q8_What programming language would you recommend an aspiring data scientist to learn first? - Selected Choice',\n",
    "              'Q13_Approximately how many times have you used a TPU (tensor processing unit)?',\n",
    "              'Q15_For how many years have you used machine learning methods?',\n",
    "              'Q20_What is the size of the company where you are employed?',\n",
    "              'Q22_Does your current employer incorporate machine learning methods into their business?',\n",
    "              'Q24_What is your current yearly compensation (approximate $USD)?',\n",
    "              'Q30_Which of the following big data products (relational database, data warehouse, data lake, or similar) do you use most often? - Selected Choice',\n",
    "              'Q32_Which of the following business intelligence tools do you use most often? - Selected Choice']\n",
    "\n",
    "# Filter rules\n",
    "regex_pattern_str = '|'.join(col_string)\n",
    "\n",
    "# Filter all Boolean columns\n",
    "df_string = df[[col for col in col_string if col in df.columns]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1752761890624,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "KWRDK6oYShXr",
    "outputId": "4bdd76cb-369a-4d97-9e1a-2759bf62c3a3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\585003900.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_string.rename(columns={old_name: new_name}, inplace=True)\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\585003900.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_string.rename(columns={old_name: new_name}, inplace=True)\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\585003900.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_string.rename(columns={old_name: new_name}, inplace=True)\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\585003900.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_string.rename(columns={old_name: new_name}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# OK, this is confusing but effective code. To be corrected later.\n",
    "\n",
    "# Q5_Select the title most similar to your current role\n",
    "old_name = df_string.columns[2]\n",
    "new_name = \"Q5_Select the title most similar to your current role\"\n",
    "df_string.rename(columns={old_name: new_name}, inplace=True)\n",
    "\n",
    "# Q8_What programming language would you recommend an aspiring data scientist to learn first?\n",
    "old_name = df_string.columns[4]\n",
    "new_name = \"Q8_What programming language would you recommend an aspiring data scientist to learn first?\"\n",
    "df_string.rename(columns={old_name: new_name}, inplace=True)\n",
    "\n",
    "# Q30_Which of the following big data products (relational database, data warehouse, data lake, or similar) do you use most often?\n",
    "old_name = df_string.columns[10]\n",
    "new_name = \"Q30_Which of the following big data products (relational database, data warehouse, data lake, or similar) do you use most often?\"\n",
    "df_string.rename(columns={old_name: new_name}, inplace=True)\n",
    "\n",
    "# Q32_Which of the following business intelligence tools do you use most often?\n",
    "old_name = df_string.columns[11]\n",
    "new_name = 'Q32_Which of the following business intelligence tools do you use most often?'\n",
    "df_string.rename(columns={old_name: new_name}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1752761890658,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "kmpAeoHK0MdG",
    "outputId": "1ce59e99-adf8-4a85-bd5e-dcd5aa99f458"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Q1_What is your age (# years)?',\n",
       "       'Q4_What is the highest level of formal education that you have attained or plan to attain within the next 2 years?',\n",
       "       'Q5_Select the title most similar to your current role',\n",
       "       'Q6_For how many years have you been writing code and/or programming?',\n",
       "       'Q8_What programming language would you recommend an aspiring data scientist to learn first?',\n",
       "       'Q13_Approximately how many times have you used a TPU (tensor processing unit)?',\n",
       "       'Q15_For how many years have you used machine learning methods?',\n",
       "       'Q20_What is the size of the company where you are employed?',\n",
       "       'Q22_Does your current employer incorporate machine learning methods into their business?',\n",
       "       'Q24_What is your current yearly compensation (approximate $USD)?',\n",
       "       'Q30_Which of the following big data products (relational database, data warehouse, data lake, or similar) do you use most often?',\n",
       "       'Q32_Which of the following business intelligence tools do you use most often?',\n",
       "       'Q7_No. of Regular used programming languages?',\n",
       "       'Q9_No. of Specialized IDE?', 'Q14_No. of DataViz Libs or Tools?',\n",
       "       'Q16_No. of ML Framworks?', 'Q17_No. of ML algorithms?',\n",
       "       'Q18_No. of Computer Vsion methods?', 'Q19_No. of NLP methods?',\n",
       "       'Q26_A No. of Current Cloud platforms?',\n",
       "       'Q27_A No. of Current Cloud Products?',\n",
       "       'Q28_A No. of Current ML products?', 'Q29_A No. of Big Data Tools?',\n",
       "       'Q31_A No. of BI Tools used?', 'Q33_A No. of Automted ML Tools?',\n",
       "       'Q34_A No. of Auto ML Tools?', 'Q35_A No. of ML Experiment Management?',\n",
       "       'Q37_No. of Learning Platforms?'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We merge qustions with boolean and string type into one DataFrame, then we drop\n",
    "# columns with \"- Selceted - \" answers.\n",
    "\n",
    "df_short = pd.concat([df_string, df_bool], axis=1)\n",
    "df_short = df_short.drop(columns=df_short.filter(regex='Selected').columns)\n",
    "df_short.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1752761890661,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "-UpUyro5BkCW"
   },
   "outputs": [],
   "source": [
    "# We delete all rows with entries ['Other', 'Student', 'z_Not selected', 'Currently not employed ']\n",
    "df_heat = df_short[~df_short['Q5_Select the title most similar to your current role'].isin(\n",
    "    ['Other', 'Student', 'z_Not selected', 'Currently not employed'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1752761890672,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "wBLVqqV2Rynu",
    "outputId": "8dc14fd9-af61-48f4-8521-e645a4d5b269"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\4195297110.py:31: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  unique_with_rank_cl = unique_with_rank.applymap(clean_float_strings)\n"
     ]
    }
   ],
   "source": [
    "# Preparing Columns transformation\n",
    "\n",
    "## Assign colums to encoder type:\n",
    "\n",
    "columns_to_keep = [col for col in encoder_assignment[\"column\"] if col in df_short.columns]\n",
    "lab_columns  = encoder_assignment.query(\"encoder == 'lab'\")[\"column\"].tolist()\n",
    "ohe_columns  = encoder_assignment.query(\"encoder == 'ohe'\")[\"column\"].tolist()\n",
    "ord_columns  = encoder_assignment.query(\"encoder == 'ord'\")[\"column\"].tolist()\n",
    "\n",
    "\n",
    "## Generate order for categories in OrderEncoder\n",
    "### Initial list\n",
    "\n",
    "\n",
    "unique_with_rank.columns = unique_with_rank.columns.str.replace(r'\\s+', ' ',\n",
    "                                                                regex=True).str.strip()\n",
    "### Clean list from NaNs\n",
    "\n",
    "def clean_float_strings(val):\n",
    "    # Werte, die echte NaN darstellen sollen\n",
    "    if val in ['nan', 'NaN', '']:\n",
    "        return np.nan\n",
    "    try:\n",
    "        f = float(val)\n",
    "        if f.is_integer():\n",
    "            return str(int(f))  #  '0.0' → '0'\n",
    "        return str(f)           #  '2.5' bleibt '2.5'\n",
    "    except:\n",
    "        return val             # Text remains text\n",
    "\n",
    "unique_with_rank_cl = unique_with_rank.applymap(clean_float_strings)\n",
    "\n",
    "### Create finale list of ordered categories for OrdinalEncoder:\n",
    "\n",
    "unique_dict = {}\n",
    "\n",
    "for col in unique_with_rank_cl.columns:\n",
    "    cats = unique_with_rank_cl[col].dropna().unique().tolist()   # Extract all values without NaNa and remove duplicates\n",
    "    cats = [cat for cat in cats if cat != 'nan']                 # Remove nan as string\n",
    "    unique_dict[col] = cats\n",
    "\n",
    "### Final list for Ordninal Encoder\n",
    "\n",
    "categories = list(unique_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlrqZQy7nECs"
   },
   "source": [
    "# Generate Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1752761890676,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "HrJyj2YyuYsy"
   },
   "outputs": [],
   "source": [
    "# Data Set S for classical Data Science Roles vs. Software Engeneering\n",
    "\n",
    "## We now reduce the data set to the above mentioned roles only:\n",
    "selected_roles_S = [\n",
    "     'Data Scientist',\n",
    "        'Software Engineer',\n",
    "        'Data Analyst'\n",
    "        ]\n",
    "\n",
    "df_heat_S = df_heat.loc[\n",
    "    df_heat.iloc[:, 2].isin(selected_roles_S)\n",
    "    ]\n",
    "\n",
    "## Eliminate the target variables. X represents the matrix of explanatory variables for the ML model.\n",
    "X_S = df_heat_S.drop(lab_columns, axis=1)\n",
    "\n",
    "## Define y as the target for the ML model.\n",
    "y_S = df_heat_S[lab_columns[0]]\n",
    "\n",
    "## Split Train- and Test-Set:\n",
    "X_train_S, X_test_S, y_train_S, y_test_S = train_test_split(X_S,\n",
    "                                                            y_S,\n",
    "                                                            test_size=0.2,\n",
    "                                                            random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1752761890689,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "1PsAnS1m9GFk",
    "outputId": "20f95427-4cc2-439a-dfbd-4c44d3dd19ca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_16732\\757090169.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_heat_L['role_group'] = role_DS(df_heat_L, role_column)                        # Add a new column to the DataFrame with the role group classification\n"
     ]
    }
   ],
   "source": [
    "# Data Set L for for Carrer paths Data Science vx. Tech\n",
    "\n",
    "## List of job roles we want to include in the filtered DataFrame:\n",
    "selected_roles_L = [\n",
    "    'Data Scientist',\n",
    "    'Software Engineer',\n",
    "    'Data Analyst',\n",
    "    'Research Scientist',\n",
    "    'Machine Learning Engineer',\n",
    "    'Data Engineer',\n",
    "    'DBA/Database Engineer'\n",
    "]\n",
    "\n",
    "## Re-arragen the target columns such that we include the career path:\n",
    "role_column = 'Q5_Select the title most similar to your current role'\n",
    "df_heat_L = df_heat[df_heat[role_column].isin(selected_roles_L)]                 # Filter the original DataFrame to only include the selected roles\n",
    "def role_DS(dframe, col_select):                                                 # Function to classify each role into a broader role group: DS, Tech, or Business\n",
    "    ds_roles = [\n",
    "        'Data Scientist',\n",
    "        'Data Analyst',\n",
    "        'Machine Learning Engineer',\n",
    "        'Data Engineer',\n",
    "        'Research Scientist'\n",
    "    ]\n",
    "    tech_roles = [\n",
    "        'Software Engineer',\n",
    "        'DBA/Database Engineer'\n",
    "    ]\n",
    "    def map_role(role):                                                          # Internal helper function to map job title to role group\n",
    "      if role in ds_roles:\n",
    "        return 'DS'\n",
    "      elif role in tech_roles:\n",
    "        return 'Tech'\n",
    "      else:\n",
    "        return 'Other'\n",
    "    return dframe[col_select].apply(map_role)                                    # Apply the role mapping to the selected column\n",
    "\n",
    "df_heat_L['role_group'] = role_DS(df_heat_L, role_column)                        # Add a new column to the DataFrame with the role group classification\n",
    "df_heat_L = df_heat_L.drop(columns=[role_column])                                # Drop the old columns and ...\n",
    "col_to_insert = df_heat_L.pop('role_group')                                      # ... insert the new one.\n",
    "df_heat_L.insert(2, role_column, col_to_insert)\n",
    "\n",
    "\n",
    "## Eliminate the target variables. X represents the matrix of explanatory variables for the ML model.\n",
    "X_L = df_heat_L.drop(lab_columns, axis=1)\n",
    "\n",
    "# Define y as the target for the ML model.\n",
    "y_L = df_heat_L[lab_columns[0]]\n",
    "\n",
    "## Split Train- and Test-Set:\n",
    "X_train_L, X_test_L, y_train_L, y_test_L = train_test_split(X_L,\n",
    "                                                            y_L,\n",
    "                                                            test_size=0.2,\n",
    "                                                            random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvXjaFjJrwse"
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1752761890729,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "1Df8xQPG6cHA"
   },
   "outputs": [],
   "source": [
    "# Apply Label Encoder for target\n",
    "\n",
    "lab_L = LabelEncoder()\n",
    "lab_S = LabelEncoder()\n",
    "\n",
    "\n",
    "y_train_L = pd.Series(y_train_L)\n",
    "y_test_L = pd.Series(y_test_L)\n",
    "y_train_L = lab_L.fit_transform(y_train_L)\n",
    "y_test_L = lab_L.transform(y_test_L)\n",
    "\n",
    "y_train_S = pd.Series(y_train_S)\n",
    "y_test_S = pd.Series(y_test_S)\n",
    "y_train_S = lab_S.fit_transform(y_train_S)\n",
    "y_test_S = lab_S.transform(y_test_S)\n",
    "\n",
    "\n",
    "# Backward Transformation of targets of L\n",
    "y_train_L_original_labels = lab_L.inverse_transform(y_train_L)\n",
    "y_test_L_original_labels = lab_L.inverse_transform(y_test_L)\n",
    "\n",
    "\n",
    "# Backward Transformation of targets of S\n",
    "y_train_S_original_labels = lab_S.inverse_transform(y_train_S)\n",
    "y_test_S_original_labels = lab_S.inverse_transform(y_test_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1752761890744,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "9n84TtBNyFO3"
   },
   "outputs": [],
   "source": [
    "ohe_transformer = Pipeline([\n",
    "    ('ohe',   OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "ord_transformer = Pipeline([\n",
    "\n",
    "    ('ord', OrdinalEncoder(categories=categories, handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('ohe', ohe_transformer, ohe_columns),\n",
    "    ('ord', ord_transformer, ord_columns)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ytuxk7AZhji"
   },
   "source": [
    "# Model Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGsyG6aVZhji"
   },
   "source": [
    "## RandomForrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2108,
     "status": "ok",
     "timestamp": 1752761892842,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "M_-W7rEWbLYP",
    "outputId": "01563e18-73d1-42fb-aceb-016543fa5cd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for data set S with RFC (Data Science Roles)\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "     Data Analyst     0.5755    0.4053    0.4756       301\n",
      "   Data Scientist     0.6471    0.7917    0.7121       528\n",
      "Software Engineer     0.6803    0.6304    0.6544       395\n",
      "\n",
      "         accuracy                         0.6446      1224\n",
      "        macro avg     0.6343    0.6091    0.6140      1224\n",
      "     weighted avg     0.6402    0.6446    0.6353      1224\n",
      "\n",
      "\n",
      "Results for data set L  With RFC (General career paths)\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          DS     0.8174    0.9613    0.8835      1369\n",
      "        Tech     0.7022    0.2983    0.4188       419\n",
      "\n",
      "    accuracy                         0.8059      1788\n",
      "   macro avg     0.7598    0.6298    0.6511      1788\n",
      "weighted avg     0.7904    0.8059    0.7746      1788\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simon\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:242: UserWarning: Found unknown categories in columns [3] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pipe_rfc = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('rfc',  RandomForestClassifier(max_depth=50,\n",
    "                                   random_state=42,\n",
    "                                   n_estimators = 100,\n",
    "                                  class_weight='balanced'  # this is crucial\n",
    "                            ))\n",
    "])\n",
    "\n",
    "pipe_rfc_S = clone(pipe_rfc)\n",
    "pipe_rfc_L = clone(pipe_rfc)\n",
    "\n",
    "pipe_rfc_S.fit(X_train_S, y_train_S)\n",
    "pipe_rfc_L.fit(X_train_L, y_train_L)\n",
    "\n",
    "\n",
    "#---Predict and evaluate---#\n",
    "\n",
    "y_rfc_S = pipe_rfc_S.predict(X_test_S)\n",
    "y_rfc_L = pipe_rfc_L.predict(X_test_L)\n",
    "\n",
    "print(\"Results for data set S with RFC (Data Science Roles)\\n\", classification_report(y_test_S, y_rfc_S, target_names=lab_S.classes_, digits=4))\n",
    "print(\"\\nResults for data set L  With RFC (General career paths)\\n\", classification_report(y_test_L, y_rfc_L, target_names=lab_L.classes_, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 162653,
     "status": "ok",
     "timestamp": 1752762055497,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "UDSmu9FrdwNm",
    "outputId": "4616d32e-aa33-4229-e4c7-3e4626dabbd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simon\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:242: UserWarning: Found unknown categories in columns [3] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for data set S (RFC)\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "     Data Analyst     0.5593    0.5482    0.5537       301\n",
      "   Data Scientist     0.6934    0.7197    0.7063       528\n",
      "Software Engineer     0.6850    0.6608    0.6727       395\n",
      "\n",
      "         accuracy                         0.6585      1224\n",
      "        macro avg     0.6459    0.6429    0.6442      1224\n",
      "     weighted avg     0.6577    0.6585    0.6579      1224\n",
      "\n",
      "\n",
      "Results for data set L (RFC)\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          DS     0.8656    0.8612    0.8634      1369\n",
      "        Tech     0.5540    0.5632    0.5586       419\n",
      "\n",
      "    accuracy                         0.7914      1788\n",
      "   macro avg     0.7098    0.7122    0.7110      1788\n",
      "weighted avg     0.7926    0.7914    0.7920      1788\n",
      "\n",
      "Best parameters for S (RFC): {'rfc__n_estimators': 200, 'rfc__min_samples_split': 10, 'rfc__min_samples_leaf': 1, 'rfc__max_features': 'log2', 'rfc__max_depth': None, 'rfc__bootstrap': True}\n",
      "Best parameters for L (RFC): {'rfc__n_estimators': 200, 'rfc__min_samples_split': 2, 'rfc__min_samples_leaf': 4, 'rfc__max_features': 'sqrt', 'rfc__max_depth': 50, 'rfc__bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "# Define Paramter Space for RandomForestClassifier:\n",
    "\n",
    "param_dist_rfc = {\n",
    "    'rfc__n_estimators': [100, 200, 300],             # No. of trres\n",
    "    'rfc__max_depth': [10, 30, 50, None],             # Max. Depth with none as no limit\n",
    "    'rfc__min_samples_split': [2, 5, 10],             # Split of Trre\n",
    "    'rfc__min_samples_leaf': [1, 2, 4],               # Minuum amout of leafs\n",
    "    'rfc__max_features': ['sqrt', 'log2', None],      # Features for split\n",
    "    'rfc__bootstrap': [True, False]                   # Bootstrapping on/off\n",
    "}\n",
    "\n",
    "\n",
    "# RandomizedSearchCV für Set S\n",
    "random_search_rfc_S = RandomizedSearchCV(\n",
    "    clone(pipe_rfc),\n",
    "    param_distributions=param_dist_rfc,\n",
    "    n_iter=10,\n",
    "    cv=3,\n",
    "    scoring='f1_macro',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# RandomizedSearchCV für Set L\n",
    "random_search_rfc_L = RandomizedSearchCV(\n",
    "    clone(pipe_rfc),\n",
    "    param_distributions=param_dist_rfc,\n",
    "    n_iter=10,\n",
    "    cv=3,\n",
    "    scoring='f1_macro',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit auf Trainingsdaten\n",
    "random_search_rfc_S.fit(X_train_S, y_train_S)\n",
    "random_search_rfc_L.fit(X_train_L, y_train_L)\n",
    "\n",
    "# Vorhersagen\n",
    "y_pred_rfc_S = random_search_rfc_S.predict(X_test_S)\n",
    "y_pred_rfc_L = random_search_rfc_L.predict(X_test_L)\n",
    "\n",
    "# Auswertung\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Results for data set S (RFC)\\n\",\n",
    "      classification_report(y_test_S, y_pred_rfc_S, target_names=lab_S.classes_, digits=4))\n",
    "\n",
    "print(\"\\nResults for data set L (RFC)\\n\",\n",
    "      classification_report(y_test_L, y_pred_rfc_L, target_names=lab_L.classes_, digits=4))\n",
    "\n",
    "# Beste Parameter\n",
    "print(\"Best parameters for S (RFC):\", random_search_rfc_S.best_params_)\n",
    "print(\"Best parameters for L (RFC):\", random_search_rfc_L.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zpm1C9B_g7CN"
   },
   "source": [
    "## Gradient Booster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4900,
     "status": "ok",
     "timestamp": 1752762060393,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "pfurFt8yg4rQ",
    "outputId": "8fdb9af3-6e21-4d06-d383-cd0fefbdcbb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for data set S with HistGradientBoosting (Data Science Roles)\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "     Data Analyst     0.5659    0.4850    0.5224       301\n",
      "   Data Scientist     0.6840    0.7216    0.7023       528\n",
      "Software Engineer     0.6797    0.7038    0.6915       395\n",
      "\n",
      "         accuracy                         0.6577      1224\n",
      "        macro avg     0.6432    0.6368    0.6387      1224\n",
      "     weighted avg     0.6536    0.6577    0.6546      1224\n",
      "\n",
      "\n",
      "Results for data set L with HistGradientBoosting (General career paths)\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          DS     0.8349    0.9196    0.8752      1369\n",
      "        Tech     0.6071    0.4057    0.4864       419\n",
      "\n",
      "    accuracy                         0.7992      1788\n",
      "   macro avg     0.7210    0.6627    0.6808      1788\n",
      "weighted avg     0.7815    0.7992    0.7841      1788\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simon\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:242: UserWarning: Found unknown categories in columns [3] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create the HGB pipeline\n",
    "pipe_hgb = Pipeline([\n",
    "    ('preprocessing', preprocessor),  # preprocessing step\n",
    "    ('hgb', HistGradientBoostingClassifier(\n",
    "        learning_rate=0.05,\n",
    "        max_iter=200,\n",
    "        max_depth=6,\n",
    "        min_samples_leaf=20,\n",
    "        l2_regularization=1.0,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Clone the pipeline for both datasets\n",
    "pipe_hgb_S = clone(pipe_hgb)\n",
    "pipe_hgb_L = clone(pipe_hgb)\n",
    "\n",
    "# Fit the pipeline to training data\n",
    "pipe_hgb_S.fit(X_train_S, y_train_S)\n",
    "pipe_hgb_L.fit(X_train_L, y_train_L)\n",
    "\n",
    "\n",
    "# Predict on test sets\n",
    "y_hgb_S = pipe_hgb_S.predict(X_test_S)\n",
    "y_hgb_L = pipe_hgb_L.predict(X_test_L)\n",
    "\n",
    "# Print results\n",
    "print(\"Results for data set S with HistGradientBoosting (Data Science Roles)\\n\",\n",
    "      classification_report(y_test_S, y_hgb_S, target_names=lab_S.classes_, digits=4))\n",
    "\n",
    "print(\"\\nResults for data set L with HistGradientBoosting (General career paths)\\n\",\n",
    "      classification_report(y_test_L, y_hgb_L, target_names=lab_L.classes_, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 154145,
     "status": "ok",
     "timestamp": 1752762214539,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "cb3sh2E4hue2",
    "outputId": "15368d01-33e4-40d7-8cbe-b4aa3dce55b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Results for data set S after Randomized Search - HGB (Data Science Roles)\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "     Data Analyst     0.5756    0.4551    0.5083       301\n",
      "   Data Scientist     0.6911    0.7500    0.7193       528\n",
      "Software Engineer     0.6828    0.7139    0.6980       395\n",
      "\n",
      "         accuracy                         0.6658      1224\n",
      "        macro avg     0.6498    0.6397    0.6419      1224\n",
      "     weighted avg     0.6600    0.6658    0.6606      1224\n",
      "\n",
      "\n",
      "Results for data set L after Randomized Search - HGB (General career paths)\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          DS     0.8446    0.9131    0.8775      1369\n",
      "        Tech     0.6136    0.4511    0.5199       419\n",
      "\n",
      "    accuracy                         0.8048      1788\n",
      "   macro avg     0.7291    0.6821    0.6987      1788\n",
      "weighted avg     0.7905    0.8048    0.7937      1788\n",
      "\n",
      "Best parameters for S (HGB): {'hgb__min_samples_leaf': 30, 'hgb__max_iter': 200, 'hgb__max_depth': 3, 'hgb__learning_rate': 0.05, 'hgb__l2_regularization': 1.0}\n",
      "Best parameters for L (HGB): {'hgb__min_samples_leaf': 30, 'hgb__max_iter': 300, 'hgb__max_depth': 6, 'hgb__learning_rate': 0.1, 'hgb__l2_regularization': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simon\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:242: UserWarning: Found unknown categories in columns [3] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define Paramter Space for Gradient Booster:\n",
    "param_dist_hgb = {\n",
    "    'hgb__learning_rate': [0.01, 0.05, 0.1],          # shrinkage step size\n",
    "    'hgb__max_iter': [100, 200, 300],                 # number of boosting iterations\n",
    "    'hgb__max_depth': [3, 6, 9],                      # maximum depth of individual trees\n",
    "    'hgb__min_samples_leaf': [10, 20, 30],            # minimum samples per leaf\n",
    "    'hgb__l2_regularization': [0.0, 1.0, 10.0]         # L2 penalty for regularization\n",
    "}\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Randomized search for dataset S\n",
    "random_search_hgb_S = RandomizedSearchCV(\n",
    "    clone(pipe_hgb),                     # avoid shared state\n",
    "    param_distributions=param_dist_hgb,  # defined param space\n",
    "    n_iter=10,                           # number of combinations\n",
    "    cv=3,                                # 3-fold CV\n",
    "    scoring='f1_macro',                  # macro F1 score\n",
    "    random_state=42,\n",
    "    n_jobs=-1,                           # use all CPUs\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Randomized search for dataset L\n",
    "random_search_hgb_L = RandomizedSearchCV(\n",
    "    clone(pipe_hgb),\n",
    "    param_distributions=param_dist_hgb,\n",
    "    n_iter=10,\n",
    "    cv=3,\n",
    "    scoring='f1_macro',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# Fit search objects to training data\n",
    "random_search_hgb_S.fit(X_train_S, y_train_S)\n",
    "random_search_hgb_L.fit(X_train_L, y_train_L)\n",
    "\n",
    "# Predict on test sets using best models\n",
    "y_pred_hgb_S = random_search_hgb_S.predict(X_test_S)\n",
    "y_pred_hgb_L = random_search_hgb_L.predict(X_test_L)\n",
    "\n",
    "# Print classification reports\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Results for data set S after Randomized Search - HGB (Data Science Roles)\\n\",\n",
    "      classification_report(y_test_S, y_pred_hgb_S, target_names=lab_S.classes_, digits=4))\n",
    "\n",
    "print(\"\\nResults for data set L after Randomized Search - HGB (General career paths)\\n\",\n",
    "      classification_report(y_test_L, y_pred_hgb_L, target_names=lab_L.classes_, digits=4))\n",
    "\n",
    "# Print best parameters\n",
    "print(\"Best parameters for S (HGB):\", random_search_hgb_S.best_params_)\n",
    "print(\"Best parameters for L (HGB):\", random_search_hgb_L.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-ghqXaqg6Kp"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMduvJ4nZhji"
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2243,
     "status": "ok",
     "timestamp": 1752762216762,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "rV4Fm97_Zhji",
    "outputId": "b217cb96-a083-4cc3-f52f-171f89c46fe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for data set S with XGBoost (Data Science Roles)\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "     Data Analyst     0.6091    0.4917    0.5441       301\n",
      "   Data Scientist     0.6949    0.7462    0.7196       528\n",
      "Software Engineer     0.6763    0.7089    0.6922       395\n",
      "\n",
      "         accuracy                         0.6716      1224\n",
      "        macro avg     0.6601    0.6489    0.6520      1224\n",
      "     weighted avg     0.6678    0.6716    0.6676      1224\n",
      "\n",
      "\n",
      "Results for data set L  With XGBoost (General career paths)\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          DS     0.8392    0.9153    0.8756      1369\n",
      "        Tech     0.6068    0.4272    0.5014       419\n",
      "\n",
      "    accuracy                         0.8009      1788\n",
      "   macro avg     0.7230    0.6712    0.6885      1788\n",
      "weighted avg     0.7848    0.8009    0.7879      1788\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simon\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:242: UserWarning: Found unknown categories in columns [3] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create the XGBoost pipeline\n",
    "pipe_xgb = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('xgb', XGBClassifier(\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    reg_lambda=1.0,\n",
    "    min_child_weight=20,\n",
    "    objective='multi:softmax',\n",
    "    num_class=4,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42\n",
    "))\n",
    "])\n",
    "\n",
    "pipe_xgb_S = clone(pipe_xgb)\n",
    "pipe_xgb_L = clone(pipe_xgb)\n",
    "\n",
    "pipe_xgb_S.fit(X_train_S, y_train_S)\n",
    "pipe_xgb_L.fit(X_train_L, y_train_L)\n",
    "\n",
    "\n",
    "\n",
    "#---Predict and evaluate---#\n",
    "\n",
    "y_xgb_S = pipe_xgb_S.predict(X_test_S)\n",
    "y_xgb_L = pipe_xgb_L.predict(X_test_L)\n",
    "\n",
    "print(\"Results for data set S with XGBoost (Data Science Roles)\\n\", classification_report(y_test_S, y_xgb_S, target_names=lab_S.classes_, digits=4))\n",
    "print(\"\\nResults for data set L  With XGBoost (General career paths)\\n\", classification_report(y_test_L, y_xgb_L, target_names=lab_L.classes_, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 59449,
     "status": "ok",
     "timestamp": 1752762276212,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "WlXth4y01s3c",
    "outputId": "c5607649-c672-43e6-d20f-2a9571dfa7b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Results for data set S after Randomized Serach  - XGB (Data Science Roles)\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "     Data Analyst     0.5907    0.4651    0.5204       301\n",
      "   Data Scientist     0.6850    0.7538    0.7178       528\n",
      "Software Engineer     0.6872    0.7063    0.6966       395\n",
      "\n",
      "         accuracy                         0.6675      1224\n",
      "        macro avg     0.6543    0.6417    0.6449      1224\n",
      "     weighted avg     0.6625    0.6675    0.6624      1224\n",
      "\n",
      "\n",
      "Results for data set L after Randomized Serach - XGB (General career paths)\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          DS     0.8448    0.9145    0.8783      1369\n",
      "        Tech     0.6176    0.4511    0.5214       419\n",
      "\n",
      "    accuracy                         0.8059      1788\n",
      "   macro avg     0.7312    0.6828    0.6998      1788\n",
      "weighted avg     0.7916    0.8059    0.7947      1788\n",
      "\n",
      "Best parameters for S (XGB): {'xgb__reg_lambda': 1.0, 'xgb__n_estimators': 200, 'xgb__min_child_weight': 10, 'xgb__max_depth': 3, 'xgb__learning_rate': 0.1}\n",
      "Best parameters for L:(XGB) {'xgb__reg_lambda': 2.0, 'xgb__n_estimators': 300, 'xgb__min_child_weight': 20, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simon\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:242: UserWarning: Found unknown categories in columns [3] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'xgb__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'xgb__n_estimators': [100, 200, 300],\n",
    "    'xgb__max_depth': [3, 6, 9],\n",
    "    'xgb__reg_lambda': [0.5, 1.0, 2.0],\n",
    "    'xgb__min_child_weight': [1, 10, 20]\n",
    "}\n",
    "\n",
    "# Create RandomizedSearchCV objects for each dataset\n",
    "random_search_S = RandomizedSearchCV(\n",
    "    clone(pipe_xgb),          # clone to avoid shared state\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,                # number of parameter settings sampled\n",
    "    cv=3,                     # 3-fold cross-validation\n",
    "    scoring='f1_macro',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,                # use all CPU cores\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "random_search_L = RandomizedSearchCV(\n",
    "    clone(pipe_xgb),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,\n",
    "    cv=3,\n",
    "    scoring='f1_macro',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the RandomizedSearchCV to training data\n",
    "random_search_S.fit(X_train_S, y_train_S)\n",
    "random_search_L.fit(X_train_L, y_train_L)\n",
    "\n",
    "# Predict on test sets using best found models\n",
    "y_pred_S = random_search_S.predict(X_test_S)\n",
    "y_pred_L = random_search_L.predict(X_test_L)\n",
    "\n",
    "# Print classification reports with original label names\n",
    "print(\"Results for data set S after Randomized Serach  - XGB (Data Science Roles)\\n\",\n",
    "      classification_report(y_test_S, y_pred_S, target_names=lab_S.classes_, digits=4))\n",
    "\n",
    "print(\"\\nResults for data set L after Randomized Serach - XGB (General career paths)\\n\",\n",
    "      classification_report(y_test_L, y_pred_L, target_names=lab_L.classes_, digits=4))\n",
    "\n",
    "# Optionally print best parameters found by the search\n",
    "print(\"Best parameters for S (XGB):\", random_search_S.best_params_)\n",
    "print(\"Best parameters for L:(XGB)\", random_search_L.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjQl-clLNDW6"
   },
   "source": [
    "# Final ML Model: Optimized XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4104,
     "status": "ok",
     "timestamp": 1752762280312,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "431pKaqZOPbV",
    "outputId": "f3e8a428-0841-4766-cb37-4c75c3aa7a34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for data set S with XGBoost (Data Science Roles)\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "     Data Analyst     0.5907    0.4651    0.5204       301\n",
      "   Data Scientist     0.6850    0.7538    0.7178       528\n",
      "Software Engineer     0.6872    0.7063    0.6966       395\n",
      "\n",
      "         accuracy                         0.6675      1224\n",
      "        macro avg     0.6543    0.6417    0.6449      1224\n",
      "     weighted avg     0.6625    0.6675    0.6624      1224\n",
      "\n",
      "\n",
      "Results for data set L  With XGBoost (General career paths)\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          DS     0.8448    0.9145    0.8783      1369\n",
      "        Tech     0.6176    0.4511    0.5214       419\n",
      "\n",
      "    accuracy                         0.8059      1788\n",
      "   macro avg     0.7312    0.6828    0.6998      1788\n",
      "weighted avg     0.7916    0.8059    0.7947      1788\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simon\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:242: UserWarning: Found unknown categories in columns [3] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the initial pipeline with default XGBoost parameters\n",
    "pipe_xgb = Pipeline([\n",
    "    ('preprocessing', preprocessor), # First step: apply preprocessing transformations\n",
    "    ('xgb', XGBClassifier(\n",
    "        # These parameters will be overridden later with the best found parameters.\n",
    "        # They serve as initial defaults or fallback values.\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=200,\n",
    "        max_depth=6,\n",
    "        reg_lambda=1.0,\n",
    "        min_child_weight=20,\n",
    "        objective='multi:softmax', # Objective for multi-class classification\n",
    "        num_class=4,             # Number of target classes\n",
    "        eval_metric='mlogloss',  # Evaluation metric for monitoring training\n",
    "        random_state=42          # Seed for reproducibility\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Clone the initial pipeline to create separate instances for datasets S and L.\n",
    "# This ensures that 'pipe_xgb_S' and 'pipe_xgb_L' are independent.\n",
    "pipe_xgb_S = clone(pipe_xgb)\n",
    "pipe_xgb_L = clone(pipe_xgb)\n",
    "\n",
    "# Define the optimal hyperparameters found for Dataset S through Randomized Search\n",
    "best_params_S = {\n",
    "    'xgb__reg_lambda': 1.0,         # L2 regularization term\n",
    "    'xgb__n_estimators': 200,       # Number of boosting rounds (trees)\n",
    "    'xgb__min_child_weight': 10,    # Minimum sum of instance weight (hessian) needed in a child\n",
    "    'xgb__max_depth': 3,            # Maximum depth of a tree\n",
    "    'xgb__learning_rate': 0.1       # Step size shrinkage to prevent overfitting\n",
    "}\n",
    "\n",
    "# Define the optimal hyperparameters found for Dataset L through Randomized Search\n",
    "best_params_L = {\n",
    "    'xgb__reg_lambda': 2.0,\n",
    "    'xgb__n_estimators': 300,\n",
    "    'xgb__min_child_weight': 20,\n",
    "    'xgb__max_depth': 6,\n",
    "    'xgb__learning_rate': 0.1\n",
    "}\n",
    "\n",
    "# Apply the optimal hyperparameters to the 'pipe_xgb_S' pipeline.\n",
    "# The 'xgb__' prefix tells set_params that these are parameters for the 'xgb' step.\n",
    "pipe_xgb_S.set_params(**best_params_S)\n",
    "\n",
    "# Apply the optimal hyperparameters to the 'pipe_xgb_L' pipeline.\n",
    "pipe_xgb_L.set_params(**best_params_L)\n",
    "\n",
    "\n",
    "# #---Train the models---#\n",
    "\n",
    "pipe_xgb_S.fit(X_train_S, y_train_S)\n",
    "pipe_xgb_L.fit(X_train_L, y_train_L)\n",
    "\n",
    "# #---Predict and evaluate---#\n",
    "\n",
    "y_xgb_S = pipe_xgb_S.predict(X_test_S)\n",
    "y_xgb_L = pipe_xgb_L.predict(X_test_L)\n",
    "\n",
    "print(\"Results for data set S with XGBoost (Data Science Roles)\\n\", classification_report(y_test_S, y_xgb_S, target_names=lab_S.classes_, digits=4))\n",
    "print(\"\\nResults for data set L  With XGBoost (General career paths)\\n\", classification_report(y_test_L, y_xgb_L, target_names=lab_L.classes_, digits=4))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1aXgwAsFPNCo-I_3AmfHDDl-mbMFuqmDf",
     "timestamp": 1749404422049
    },
    {
     "file_id": "1JCjLAqw1GA0s2f1A-BUPI-7Cp9wUZXd6",
     "timestamp": 1748885543791
    },
    {
     "file_id": "1jmdoFNwP29X0PTg4OqT3DPkIkBXkP9jl",
     "timestamp": 1748094260592
    },
    {
     "file_id": "1bw1oLViHjd-k2wztUnu8xRvlA5hPsov_",
     "timestamp": 1747054697519
    },
    {
     "file_id": "1N_iIIcytq2esfepkk9CVbbWpZYKTzE5a",
     "timestamp": 1743324642691
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
