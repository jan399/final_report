{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdfJ53H2zo4c"
   },
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTK3FuTTZxxm"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1753262602068,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "kkB-xJWBPsqj",
    "outputId": "0c5c8764-798d-46bc-ce2e-68255e5812dd"
   },
   "outputs": [],
   "source": [
    "#Import of relevant libraries, classes or methods (in order of apperance)\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import joblib\n",
    "import shap\n",
    "import json\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current notebook directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Go one level up and build the path to the data folder\n",
    "\n",
    "data_path_df = os.path.join(current_dir, \"..\", \"data\", \n",
    "                         \"kaggle_survey_2020_responses.csv\")\n",
    "\n",
    "data_path_encoder_assignment = os.path.join(current_dir, \"..\", \"data\", \n",
    "                         \"encoder_assignment.csv\")\n",
    "\n",
    "data_path_unique_with_rank = os.path.join(current_dir, \"..\", \"data\", \n",
    "                         \"unique_with_rank.csv\")\n",
    "\n",
    "\n",
    "# Load the CSV files and instantiate DataFrames\n",
    "\n",
    "df                 = pd.read_csv(data_path_df,\n",
    "                                 dtype = 'str')\n",
    "\n",
    "encoder_assignment = pd.read_csv(data_path_encoder_assignment,\n",
    "                                 sep = ';')\n",
    "\n",
    "\n",
    "unique_with_rank = pd.read_csv(data_path_unique_with_rank, \n",
    "                               dtype = 'str',\n",
    "                               sep = ';',\n",
    "                               na_values=['nan', '', 'NaN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xANjAbYFaJ3j"
   },
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "N11d-rFuSwQW"
   },
   "outputs": [],
   "source": [
    "# Merge the question numbers with the question text for better readibility\n",
    " # We will shorten the column names later.\n",
    "\n",
    "col_index = df.columns.tolist()\n",
    "# print(col_index)\n",
    "\n",
    "questions = df.iloc[0,:].tolist()\n",
    "# print(questions)\n",
    "\n",
    "# Generate the composed index in a new list. Use a list comprehenshion with built in zip function:\n",
    "col_index_new = [f\"{col}_{val}\" for col, val in zip(col_index, questions)]\n",
    "\n",
    "#  print(col_index_new)\n",
    "\n",
    "# Step 2: Assign the new index to the DataFrame\n",
    "df.columns = col_index_new\n",
    "\n",
    "# Step 3: Delete the first row (no longer used)\n",
    "df = df.iloc[1:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file successfully saved to: C:\\Users\\Simon\\Documents\\GitHub\\RoleRecommender\\final_report\\notebooks\\downloads_for_streamlit\\df_long.csv\n"
     ]
    }
   ],
   "source": [
    "###---EXPORT FOR STREMLIT---###\n",
    "# Get the current notebook directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Build the path to the 'downloads_for_streamlit' folder\n",
    "export_path = os.path.join(current_dir, \"downloads_for_streamlit\", \"df_long.csv\")\n",
    "\n",
    "# Export the DataFrame as CSV\n",
    "df.to_csv(export_path, sep=';', index=False)\n",
    "\n",
    "# Test output\n",
    "print(f\"CSV file successfully saved to: {export_path}\")\n",
    "###---EXPORT FOR STREMLIT---###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JASao2C0qH-m"
   },
   "outputs": [],
   "source": [
    "# Within the 'Selected Choice'-columns, it is better to transform the values\n",
    "# into Booleans, because they are better to count and better to modell.\n",
    "# All others are strings. An NaN is interpreted as \"z_not selected\"\n",
    "\n",
    "mark_bool = '- Selected Choice -'                     # If the questions have \"- Seleceted choice - \", the answer ist yes or no (Boolean)\n",
    "\n",
    "\n",
    "for col in df.columns:                                # NaNs are interpreted as False\n",
    "    if mark_bool in col:\n",
    "        df[col] = df[col].apply(lambda x:\n",
    "                                False if pd.isna(x)\n",
    "                                else True)            # If it's not a NaN, the answer is given. We replace it with a True\n",
    "    else:\n",
    "        df[col] = df[col].fillna('z_Not selected')     # NaNs are interpreted as \"z_Not selected\". z as a prefix to secure that it is always on the right hand side of the plot.\n",
    "        df[col] = df[col].astype(str)                  # The whole columns is transformed into a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_Jm8yi_KXRLP"
   },
   "outputs": [],
   "source": [
    "# We now choose the questions of boolean type and assign them do a DataFram df_bool::\n",
    "\n",
    "# Questions with boolean character:\n",
    "\n",
    "col_bool = ['^Q7', '^Q9', '^Q14', '^Q16', '^Q17', '^Q18', '^Q19',\n",
    "            '^Q26_A', '^Q27_A', '^Q28_A', '^Q29_A', '^Q31_A', '^Q33_A', '^Q34_A', '^Q35_A', '^Q37']\n",
    "\n",
    "# Filter rules\n",
    "regex_pattern_bool = '|'.join(col_bool)\n",
    "\n",
    "# Filter all Boolean columns\n",
    "df_bool = df.filter(regex=regex_pattern_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 204,
     "status": "ok",
     "timestamp": 1753261127125,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "Y0zAtZ4_Xrhf",
    "outputId": "16905116-4185-4e46-dd13-330bc69bda5d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\178850794.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\178850794.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\178850794.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\178850794.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\178850794.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\178850794.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\178850794.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\178850794.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\178850794.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\178850794.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\178850794.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\178850794.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\178850794.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\178850794.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\178850794.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\178850794.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\178850794.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\178850794.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\178850794.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\178850794.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\178850794.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\178850794.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\178850794.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\178850794.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\178850794.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\178850794.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\178850794.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\178850794.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\178850794.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\178850794.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\178850794.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\178850794.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n"
     ]
    }
   ],
   "source": [
    "# Define a pattern to sum up the relevant columns\n",
    "\n",
    "# Mapping Regex → Columns for summing up\n",
    "sum_columns = {\n",
    "    '^Q7':    'Q7_No. of Regular used programming languages?',\n",
    "    '^Q9':    'Q9_No. of Specialized IDE?',\n",
    "    '^Q14':   'Q14_No. of DataViz Libs or Tools?',\n",
    "    '^Q16':   'Q16_No. of ML Framworks?',\n",
    "    '^Q17':   'Q17_No. of ML algorithms?',\n",
    "    '^Q18':   'Q18_No. of Computer Vsion methods?',\n",
    "    '^Q19':   'Q19_No. of NLP methods?',\n",
    "    '^Q26_A': 'Q26_A No. of Current Cloud platforms?',\n",
    "    '^Q27_A': 'Q27_A No. of Current Cloud Products?',\n",
    "    '^Q28_A': 'Q28_A No. of Current ML products?',\n",
    "    '^Q29_A': 'Q29_A No. of Big Data Tools?',\n",
    "    '^Q31_A': 'Q31_A No. of BI Tools used?',\n",
    "    '^Q33_A': 'Q33_A No. of Automted ML Tools?',\n",
    "    '^Q34_A': 'Q34_A No. of Auto ML Tools?',\n",
    "    '^Q35_A': 'Q35_A No. of ML Experiment Management?',\n",
    "    '^Q37':   'Q37_No. of Learning Platforms?'\n",
    "\n",
    "}\n",
    "\n",
    "for regex, new_col in sum_columns.items():\n",
    "    matching_cols = df.filter(regex=regex).columns                                         # Select all columns that match\n",
    "    valid_cols = [col for col in matching_cols if \"- Selected Choice - None\" not in col]   # Exclude columns with \"none\"\n",
    "    df_bool[new_col] = df[valid_cols].sum(axis=1)                                          # sum up valid columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "SR_jNhq25MG9"
   },
   "outputs": [],
   "source": [
    "# We now choose the questions of string type (multiple choice) and assign them do a DataFram df_string::\n",
    "\n",
    "col_string = ['Q1_What is your age (# years)?',\n",
    "              'Q4_What is the highest level of formal education that you have attained or plan to attain within the next 2 years?',\n",
    "              'Q5_Select the title most similar to your current role (or most recent title if retired): - Selected Choice',\n",
    "              'Q6_For how many years have you been writing code and/or programming?',\n",
    "              'Q8_What programming language would you recommend an aspiring data scientist to learn first? - Selected Choice',\n",
    "              'Q13_Approximately how many times have you used a TPU (tensor processing unit)?',\n",
    "              'Q15_For how many years have you used machine learning methods?',\n",
    "              'Q20_What is the size of the company where you are employed?',\n",
    "              'Q22_Does your current employer incorporate machine learning methods into their business?',\n",
    "              'Q24_What is your current yearly compensation (approximate $USD)?',\n",
    "              'Q30_Which of the following big data products (relational database, data warehouse, data lake, or similar) do you use most often? - Selected Choice',\n",
    "              'Q32_Which of the following business intelligence tools do you use most often? - Selected Choice']\n",
    "\n",
    "# Filter rules\n",
    "regex_pattern_str = '|'.join(col_string)\n",
    "\n",
    "# Filter all Boolean columns\n",
    "df_string = df[[col for col in col_string if col in df.columns]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1753261127145,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "KWRDK6oYShXr",
    "outputId": "c91dfdb3-6472-4be8-fcb4-37de2f400ee5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\585003900.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_string.rename(columns={old_name: new_name}, inplace=True)\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\585003900.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_string.rename(columns={old_name: new_name}, inplace=True)\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\585003900.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_string.rename(columns={old_name: new_name}, inplace=True)\n",
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\585003900.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_string.rename(columns={old_name: new_name}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# OK, this is confusing but effective code. To be corrected later.\n",
    "\n",
    "# Q5_Select the title most similar to your current role\n",
    "old_name = df_string.columns[2]\n",
    "new_name = \"Q5_Select the title most similar to your current role\"\n",
    "df_string.rename(columns={old_name: new_name}, inplace=True)\n",
    "\n",
    "# Q8_What programming language would you recommend an aspiring data scientist to learn first?\n",
    "old_name = df_string.columns[4]\n",
    "new_name = \"Q8_What programming language would you recommend an aspiring data scientist to learn first?\"\n",
    "df_string.rename(columns={old_name: new_name}, inplace=True)\n",
    "\n",
    "# Q30_Which of the following big data products (relational database, data warehouse, data lake, or similar) do you use most often?\n",
    "old_name = df_string.columns[10]\n",
    "new_name = \"Q30_Which of the following big data products (relational database, data warehouse, data lake, or similar) do you use most often?\"\n",
    "df_string.rename(columns={old_name: new_name}, inplace=True)\n",
    "\n",
    "# Q32_Which of the following business intelligence tools do you use most often?\n",
    "old_name = df_string.columns[11]\n",
    "new_name = 'Q32_Which of the following business intelligence tools do you use most often?'\n",
    "df_string.rename(columns={old_name: new_name}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 68,
     "status": "ok",
     "timestamp": 1753261127213,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "kmpAeoHK0MdG",
    "outputId": "dca1286c-23df-4c70-a5fa-61f865495711"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Q1_What is your age (# years)?',\n",
       "       'Q4_What is the highest level of formal education that you have attained or plan to attain within the next 2 years?',\n",
       "       'Q5_Select the title most similar to your current role',\n",
       "       'Q6_For how many years have you been writing code and/or programming?',\n",
       "       'Q8_What programming language would you recommend an aspiring data scientist to learn first?',\n",
       "       'Q13_Approximately how many times have you used a TPU (tensor processing unit)?',\n",
       "       'Q15_For how many years have you used machine learning methods?',\n",
       "       'Q20_What is the size of the company where you are employed?',\n",
       "       'Q22_Does your current employer incorporate machine learning methods into their business?',\n",
       "       'Q24_What is your current yearly compensation (approximate $USD)?',\n",
       "       'Q30_Which of the following big data products (relational database, data warehouse, data lake, or similar) do you use most often?',\n",
       "       'Q32_Which of the following business intelligence tools do you use most often?',\n",
       "       'Q7_No. of Regular used programming languages?',\n",
       "       'Q9_No. of Specialized IDE?', 'Q14_No. of DataViz Libs or Tools?',\n",
       "       'Q16_No. of ML Framworks?', 'Q17_No. of ML algorithms?',\n",
       "       'Q18_No. of Computer Vsion methods?', 'Q19_No. of NLP methods?',\n",
       "       'Q26_A No. of Current Cloud platforms?',\n",
       "       'Q27_A No. of Current Cloud Products?',\n",
       "       'Q28_A No. of Current ML products?', 'Q29_A No. of Big Data Tools?',\n",
       "       'Q31_A No. of BI Tools used?', 'Q33_A No. of Automted ML Tools?',\n",
       "       'Q34_A No. of Auto ML Tools?', 'Q35_A No. of ML Experiment Management?',\n",
       "       'Q37_No. of Learning Platforms?'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We merge qustions with boolean and string type into one DataFrame, then we drop\n",
    "# columns with \"- Selceted - \" answers.\n",
    "\n",
    "df_short = pd.concat([df_string, df_bool], axis=1)\n",
    "df_short = df_short.drop(columns=df_short.filter(regex='Selected').columns)\n",
    "df_short.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "-UpUyro5BkCW"
   },
   "outputs": [],
   "source": [
    "# We delete all rows with entries ['Other', 'Student', 'z_Not selected', 'Currently not employed ']\n",
    "df_heat = df_short[~df_short['Q5_Select the title most similar to your current role'].isin(\n",
    "    ['Other', 'Student', 'z_Not selected', 'Currently not employed'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1753261127272,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "wBLVqqV2Rynu",
    "outputId": "ea776c73-7993-41b9-a919-60a8b99ebb76"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\4195297110.py:31: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  unique_with_rank_cl = unique_with_rank.applymap(clean_float_strings)\n"
     ]
    }
   ],
   "source": [
    "# Preparing Columns transformation\n",
    "\n",
    "## Assign colums to encoder type:\n",
    "\n",
    "columns_to_keep = [col for col in encoder_assignment[\"column\"] if col in df_short.columns]\n",
    "lab_columns  = encoder_assignment.query(\"encoder == 'lab'\")[\"column\"].tolist()\n",
    "ohe_columns  = encoder_assignment.query(\"encoder == 'ohe'\")[\"column\"].tolist()\n",
    "ord_columns  = encoder_assignment.query(\"encoder == 'ord'\")[\"column\"].tolist()\n",
    "\n",
    "\n",
    "## Generate order for categories in OrderEncoder\n",
    "### Initial list\n",
    "\n",
    "\n",
    "unique_with_rank.columns = unique_with_rank.columns.str.replace(r'\\s+', ' ',\n",
    "                                                                regex=True).str.strip()\n",
    "### Clean list from NaNs\n",
    "\n",
    "def clean_float_strings(val):\n",
    "    # Werte, die echte NaN darstellen sollen\n",
    "    if val in ['nan', 'NaN', '']:\n",
    "        return np.nan\n",
    "    try:\n",
    "        f = float(val)\n",
    "        if f.is_integer():\n",
    "            return str(int(f))  #  '0.0' → '0'\n",
    "        return str(f)           #  '2.5' bleibt '2.5'\n",
    "    except:\n",
    "        return val             # Text remains text\n",
    "\n",
    "unique_with_rank_cl = unique_with_rank.applymap(clean_float_strings)\n",
    "\n",
    "### Create finale list of ordered categories for OrdinalEncoder:\n",
    "\n",
    "unique_dict = {}\n",
    "\n",
    "for col in unique_with_rank_cl.columns:\n",
    "    cats = unique_with_rank_cl[col].dropna().unique().tolist()   # Extract all values without NaNa and remove duplicates\n",
    "    cats = [cat for cat in cats if cat != 'nan']                 # Remove nan as string\n",
    "    unique_dict[col] = cats\n",
    "\n",
    "### Final list for Ordninal Encoder\n",
    "\n",
    "categories = list(unique_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file successfully saved to: C:\\Users\\Simon\\Documents\\GitHub\\RoleRecommender\\final_report\\notebooks\\downloads_for_streamlit\\categories.json\n"
     ]
    }
   ],
   "source": [
    "###---EXPORT FOR STREAMLIT---###\n",
    "# Get the current notebook directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Build the path to the 'downloads_for_streamlit' folder\n",
    "export_path = os.path.join(current_dir, \"downloads_for_streamlit\", \"categories.json\")\n",
    "\n",
    "# Save the list as JSON\n",
    "with open(export_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(categories, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Test output\n",
    "print(f\"JSON file successfully saved to: {export_path}\")\n",
    "###---EXPORT FOR STREAMLIT---###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file successfully saved to: C:\\Users\\Simon\\Documents\\GitHub\\RoleRecommender\\final_report\\notebooks\\downloads_for_streamlit\\ohe_columns.json\n"
     ]
    }
   ],
   "source": [
    "###---EXPORT FOR STREAMLIT---###\n",
    "# Get the current notebook directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Build the path to the 'downloads_for_streamlit' folder\n",
    "export_path = os.path.join(current_dir, \"downloads_for_streamlit\", \"ohe_columns.json\")\n",
    "\n",
    "# Save the list as JSON\n",
    "with open(export_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(ohe_columns, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Test output\n",
    "print(f\"JSON file successfully saved to: {export_path}\")\n",
    "###---EXPORT FOR STREAMLIT---###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file successfully saved to: C:\\Users\\Simon\\Documents\\GitHub\\RoleRecommender\\final_report\\notebooks\\downloads_for_streamlit\\ord_columns.json\n"
     ]
    }
   ],
   "source": [
    "###---EXPORT FOR STREAMLIT---###\n",
    "# Get the current notebook directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Build the path to the 'downloads_for_streamlit' folder\n",
    "export_path = os.path.join(current_dir, \"downloads_for_streamlit\", \"ord_columns.json\")\n",
    "\n",
    "# Save the list as JSON\n",
    "with open(export_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(ord_columns, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Test output\n",
    "print(f\"JSON file successfully saved to: {export_path}\")\n",
    "###---EXPORT FOR STREAMLIT---###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlrqZQy7nECs"
   },
   "source": [
    "# Generate Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "HrJyj2YyuYsy"
   },
   "outputs": [],
   "source": [
    "# Data Set S for classical Data Science Roles vs. Software Engeneering\n",
    "\n",
    "## We now reduce the data set to the above mentioned roles only:\n",
    "selected_roles_S = [\n",
    "     'Data Scientist',\n",
    "        'Software Engineer',\n",
    "        'Data Analyst'\n",
    "        ]\n",
    "\n",
    "df_heat_S = df_heat.loc[\n",
    "    df_heat.iloc[:, 2].isin(selected_roles_S)\n",
    "    ]\n",
    "\n",
    "## Eliminate the target variables. X represents the matrix of explanatory variables for the ML model.\n",
    "X_S = df_heat_S.drop(lab_columns, axis=1)\n",
    "\n",
    "## Define y as the target for the ML model.\n",
    "y_S = df_heat_S[lab_columns[0]]\n",
    "\n",
    "## Split Train- and Test-Set:\n",
    "X_train_S, X_test_S, y_train_S, y_test_S = train_test_split(X_S,\n",
    "                                                            y_S,\n",
    "                                                            test_size=0.2,\n",
    "                                                            random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 71,
     "status": "ok",
     "timestamp": 1753261127530,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "-vN5dLnWNOU-",
    "outputId": "ebe64a39-4e6c-430a-daff-58772b89133d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file successfully saved to: C:\\Users\\Simon\\Documents\\GitHub\\RoleRecommender\\final_report\\notebooks\\downloads_for_streamlit\\df_heat_S.csv\n"
     ]
    }
   ],
   "source": [
    "###---EXPORT FOR STREAMLIT---###\n",
    "# Get the current notebook directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Build the path to the 'downloads_for_streamlit' folder\n",
    "export_path = os.path.join(current_dir, \"downloads_for_streamlit\", \"df_heat_S.csv\")\n",
    "\n",
    "# Export the DataFrame as CSV\n",
    "df_heat_S.to_csv(export_path, sep=';', index=False)\n",
    "\n",
    "# Test output\n",
    "print(f\"CSV file successfully saved to: {export_path}\")\n",
    "###---EXPORT FOR STREAMLIT---###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1753261127553,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "0Idb-hH_JGw8",
    "outputId": "b820dc3a-639a-4e3c-b80e-8bdefb4b1e62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file successfully saved to: C:\\Users\\Simon\\Documents\\GitHub\\RoleRecommender\\final_report\\notebooks\\downloads_for_streamlit\\default_X_train_S.csv\n"
     ]
    }
   ],
   "source": [
    "###---EXPORT FOR STREAMLIT---###\n",
    "# Get the current notebook directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Build the path to the 'downloads_for_streamlit' folder\n",
    "export_path = os.path.join(current_dir, \"downloads_for_streamlit\", \"default_X_train_S.csv\")\n",
    "\n",
    "# Create the default_X_train_S DataFrame\n",
    "default_X_train_S = X_train_S.mode().iloc[0:1]\n",
    "\n",
    "# Export the DataFrame as CSV\n",
    "default_X_train_S.to_csv(export_path, sep=';', index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Test output\n",
    "print(f\"CSV file successfully saved to: {export_path}\")\n",
    "###---EXPORT FOR STREAMLIT---###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1753261127565,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "1rsF9oRd4ZPu",
    "outputId": "1ba78a0d-8236-4420-99e1-74de030d29a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file successfully saved to: C:\\Users\\Simon\\Documents\\GitHub\\RoleRecommender\\final_report\\notebooks\\downloads_for_streamlit\\unique_values_per_feature_S.json\n"
     ]
    }
   ],
   "source": [
    "###---EXPORT FOR STREAMLIT (S)---###\n",
    "# Get the current notebook directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Build the path to the 'downloads_for_streamlit' folder\n",
    "export_path = os.path.join(current_dir, \"downloads_for_streamlit\", \"unique_values_per_feature_S.json\")\n",
    "\n",
    "# Generate the dictionary of unique values\n",
    "def extract_unique_values(df):\n",
    "    return {col: sorted(df[col].dropna().unique().astype(str).tolist()) for col in df.columns}\n",
    "\n",
    "unique_map_S = extract_unique_values(df_heat_S)\n",
    "\n",
    "# Save as JSON\n",
    "with open(export_path, \"w\", encoding=\"utf-8-sig\") as f:\n",
    "    json.dump(unique_map_S, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Test output\n",
    "print(f\"JSON file successfully saved to: {export_path}\")\n",
    "###---EXPORT FOR STREAMLIT (S)---###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1753261127579,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "1PsAnS1m9GFk",
    "outputId": "3b35dc11-0b12-4ef4-91da-dcfbf4e4a760"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_12448\\134686113.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_heat_L['role_group'] = role_DS(df_heat_L, role_column)                        # Add a new column to the DataFrame with the role group classification\n"
     ]
    }
   ],
   "source": [
    "# Data Set L for for Carrer paths Data Science vx. Tech\n",
    "\n",
    "## List of job roles we want to include in the filtered DataFrame:\n",
    "selected_roles_L = [\n",
    "    'Data Scientist',\n",
    "    'Software Engineer',\n",
    "    'Data Analyst',\n",
    "    'Research Scientist',\n",
    "    'Machine Learning Engineer',\n",
    "    'Data Engineer',\n",
    "    'DBA/Database Engineer'\n",
    "]\n",
    "\n",
    "## Re-arragen the target columns such that we include the career path:\n",
    "role_column = 'Q5_Select the title most similar to your current role'\n",
    "df_heat_L = df_heat[df_heat[role_column].isin(selected_roles_L)]                 # Filter the original DataFrame to only include the selected roles\n",
    "def role_DS(dframe, col_select):                                                 # Function to classify each role into a broader role group: DS, Tech, or Business\n",
    "    ds_roles = [\n",
    "        'Data Scientist',\n",
    "        'Data Analyst',\n",
    "        'Machine Learning Engineer',\n",
    "        'Data Engineer',\n",
    "        'Research Scientist'\n",
    "    ]\n",
    "    tech_roles = [\n",
    "        'Software Engineer',\n",
    "        'DBA/Database Engineer'\n",
    "    ]\n",
    "    def map_role(role):                                                          # Internal helper function to map job title to role group\n",
    "      if role in ds_roles:\n",
    "        return 'DS'\n",
    "      elif role in tech_roles:\n",
    "        return 'Tech'\n",
    "      else:\n",
    "        return 'Other'\n",
    "    return dframe[col_select].apply(map_role)                                    # Apply the role mapping to the selected column\n",
    "\n",
    "\n",
    "df_heat_L['role_group'] = role_DS(df_heat_L, role_column)                        # Add a new column to the DataFrame with the role group classification\n",
    "df_heat_L = df_heat_L.drop(columns=[role_column])                                # Drop the old columns and ...\n",
    "col_to_insert = df_heat_L.pop('role_group')                                      # ... insert the new one.\n",
    "df_heat_L.insert(2, role_column, col_to_insert)\n",
    "\n",
    "\n",
    "## Eliminate the target variables. X represents the matrix of explanatory variables for the ML model.\n",
    "X_L = df_heat_L.drop(lab_columns, axis=1)\n",
    "\n",
    "# Define y as the target for the ML model.\n",
    "y_L = df_heat_L[lab_columns[0]]\n",
    "\n",
    "## Split Train- and Test-Set:\n",
    "X_train_L, X_test_L, y_train_L, y_test_L = train_test_split(X_L,\n",
    "                                                            y_L,\n",
    "                                                            test_size=0.2,\n",
    "                                                            random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1753261127587,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "_sdAUoFCQKm1",
    "outputId": "d0aef8c8-6634-421f-beb7-32df56afc648"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file successfully saved to: C:\\Users\\Simon\\Documents\\GitHub\\RoleRecommender\\final_report\\notebooks\\downloads_for_streamlit\\df_heat_L.csv\n"
     ]
    }
   ],
   "source": [
    "###---EXPORT FOR STREAMLIT---###\n",
    "# Get the current notebook directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Build the path to the 'downloads_for_streamlit' folder\n",
    "export_path = os.path.join(current_dir, \"downloads_for_streamlit\", \"df_heat_L.csv\")\n",
    "\n",
    "# Export the DataFrame as CSV\n",
    "df_heat_L.to_csv(export_path, sep=';', index=False)\n",
    "\n",
    "# Test output\n",
    "print(f\"CSV file successfully saved to: {export_path}\")\n",
    "###---EXPORT FOR STREAMLIT---###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1753261127599,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "7ysMP6c00ERD",
    "outputId": "768ca86a-624a-4b97-9972-6da79e1e77e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file successfully saved to: C:\\Users\\Simon\\Documents\\GitHub\\RoleRecommender\\final_report\\notebooks\\downloads_for_streamlit\\default_X_train_L.csv\n"
     ]
    }
   ],
   "source": [
    "###---EXPORT FOR STREAMLIT---###\n",
    "# Get the current notebook directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Build the path to the 'downloads_for_streamlit' folder\n",
    "export_path = os.path.join(current_dir, \"downloads_for_streamlit\", \"default_X_train_L.csv\")\n",
    "\n",
    "# Create the default_X_train_L DataFrame\n",
    "default_X_train_L = X_train_L.mode().iloc[0:1]\n",
    "\n",
    "# Export the DataFrame as CSV\n",
    "default_X_train_L.to_csv(export_path, sep=';', index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Test output\n",
    "print(f\"CSV file successfully saved to: {export_path}\")\n",
    "###---EXPORT FOR STREAMLIT---###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1753261127604,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "XwFUmpbJ5e_K",
    "outputId": "896332e7-cc97-4ebb-d251-117116e6ea5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file successfully saved to: C:\\Users\\Simon\\Documents\\GitHub\\RoleRecommender\\final_report\\notebooks\\downloads_for_streamlit\\unique_values_per_feature_L.json\n"
     ]
    }
   ],
   "source": [
    "###---EXPORT FOR STREAMLIT (L)---###\n",
    "# Get the current notebook directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Build the path to the 'downloads_for_streamlit' folder\n",
    "export_path = os.path.join(current_dir, \"downloads_for_streamlit\", \"unique_values_per_feature_L.json\")\n",
    "\n",
    "# Generate the dictionary of unique values\n",
    "def extract_unique_values(df):\n",
    "    return {col: sorted(df[col].dropna().unique().astype(str).tolist()) for col in df.columns}\n",
    "\n",
    "unique_map_L = extract_unique_values(df_heat_L)\n",
    "\n",
    "# Save as JSON\n",
    "with open(export_path, \"w\", encoding=\"utf-8-sig\") as f:\n",
    "    json.dump(unique_map_L, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Test output\n",
    "print(f\"JSON file successfully saved to: {export_path}\")\n",
    "###---EXPORT FOR STREAMLIT (L)---###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvXjaFjJrwse"
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "1Df8xQPG6cHA"
   },
   "outputs": [],
   "source": [
    "# Apply Label Encoder for target\n",
    "\n",
    "lab_L = LabelEncoder()\n",
    "lab_S = LabelEncoder()\n",
    "\n",
    "\n",
    "y_train_L = pd.Series(y_train_L)\n",
    "y_test_L = pd.Series(y_test_L)\n",
    "y_train_L = lab_L.fit_transform(y_train_L)\n",
    "y_test_L = lab_L.transform(y_test_L)\n",
    "\n",
    "y_train_S = pd.Series(y_train_S)\n",
    "y_test_S = pd.Series(y_test_S)\n",
    "y_train_S = lab_S.fit_transform(y_train_S)\n",
    "y_test_S = lab_S.transform(y_test_S)\n",
    "\n",
    "\n",
    "# Backward Transformation of targets of L\n",
    "y_train_L_original_labels = lab_L.inverse_transform(y_train_L)\n",
    "y_test_L_original_labels = lab_L.inverse_transform(y_test_L)\n",
    "\n",
    "\n",
    "# Backward Transformation of targets of S\n",
    "y_train_S_original_labels = lab_S.inverse_transform(y_train_S)\n",
    "y_test_S_original_labels = lab_S.inverse_transform(y_test_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "9n84TtBNyFO3"
   },
   "outputs": [],
   "source": [
    "ohe_transformer = Pipeline([\n",
    "    ('ohe',   OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "ord_transformer = Pipeline([\n",
    "\n",
    "    ('ord', OrdinalEncoder(categories=categories, handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "])\n",
    "\n",
    "\n",
    "# Define preprocessors separately for S and L\n",
    "preprocessor_S = ColumnTransformer([\n",
    "    ('ohe', ohe_transformer, ohe_columns),\n",
    "    ('ord', ord_transformer, ord_columns)\n",
    "])\n",
    "\n",
    "preprocessor_L = ColumnTransformer([\n",
    "    ('ohe', ohe_transformer, ohe_columns),\n",
    "    ('ord', ord_transformer, ord_columns)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjQl-clLNDW6"
   },
   "source": [
    "# Final ML Model: Optimized XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OlDcm98USeFN"
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2079,
     "status": "ok",
     "timestamp": 1753261129722,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "431pKaqZOPbV",
    "outputId": "e9a8435d-4cd9-4df3-e888-5976871ce633"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for data set S with XGBoost (Data Science Roles)\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "     Data Analyst     0.5907    0.4651    0.5204       301\n",
      "   Data Scientist     0.6850    0.7538    0.7178       528\n",
      "Software Engineer     0.6872    0.7063    0.6966       395\n",
      "\n",
      "         accuracy                         0.6675      1224\n",
      "        macro avg     0.6543    0.6417    0.6449      1224\n",
      "     weighted avg     0.6625    0.6675    0.6624      1224\n",
      "\n",
      "\n",
      "Results for data set L  With XGBoost (General career paths)\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          DS     0.8433    0.9160    0.8782      1369\n",
      "        Tech     0.6179    0.4439    0.5167       419\n",
      "\n",
      "    accuracy                         0.8054      1788\n",
      "   macro avg     0.7306    0.6800    0.6974      1788\n",
      "weighted avg     0.7905    0.8054    0.7934      1788\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simon\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:242: UserWarning: Found unknown categories in columns [3] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "C:\\Users\\Simon\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:242: UserWarning: Found unknown categories in columns [3] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Determine the number of unique classes in each target set\n",
    "num_classes_S = len(np.unique(y_train_S))\n",
    "num_classes_L = len(np.unique(y_train_L))\n",
    "\n",
    "# Build pipelines, each with their own preprocessor\n",
    "\n",
    "# Pipeline for dataset S\n",
    "pipe_xgb_S = Pipeline([\n",
    "    ('preprocessing', preprocessor_S),     # Uses the dedicated preprocessor for S\n",
    "    ('xgb', XGBClassifier(\n",
    "        objective='multi:softmax',\n",
    "        num_class=num_classes_S,           # Set according to unique classes in y_train_S\n",
    "        eval_metric='mlogloss',\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Pipeline for dataset L\n",
    "pipe_xgb_L = Pipeline([\n",
    "    ('preprocessing', preprocessor_L),     # Uses the dedicated preprocessor for L\n",
    "    ('xgb', XGBClassifier(\n",
    "        objective='multi:softmax',\n",
    "        num_class=num_classes_L,           # Set according to unique classes in y_train_L\n",
    "        eval_metric='mlogloss',\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "# Define the optimal hyperparameters found for Dataset S through Randomized Search\n",
    "best_params_S = {\n",
    "    'xgb__reg_lambda': 1.0,         # L2 regularization term\n",
    "    'xgb__n_estimators': 200,       # Number of boosting rounds (trees)\n",
    "    'xgb__min_child_weight': 10,    # Minimum sum of instance weight (hessian) needed in a child\n",
    "    'xgb__max_depth': 3,            # Maximum depth of a tree\n",
    "    'xgb__learning_rate': 0.1       # Step size shrinkage to prevent overfitting\n",
    "}\n",
    "\n",
    "# Define the optimal hyperparameters found for Dataset L through Randomized Search\n",
    "best_params_L = {\n",
    "    'xgb__reg_lambda': 2.0,\n",
    "    'xgb__n_estimators': 300,\n",
    "    'xgb__min_child_weight': 20,\n",
    "    'xgb__max_depth': 6,\n",
    "    'xgb__learning_rate': 0.1\n",
    "}\n",
    "\n",
    "# Set optimized hyperparameters (from Randomized Search or Grid Search) for each pipeline\n",
    "pipe_xgb_S.set_params(**best_params_S)\n",
    "pipe_xgb_L.set_params(**best_params_L)\n",
    "\n",
    "# Fit/train both pipelines with their respective training data\n",
    "pipe_xgb_S.fit(X_train_S, y_train_S)\n",
    "pipe_xgb_L.fit(X_train_L, y_train_L)\n",
    "\n",
    "# #---Predict and evaluate---#\n",
    "\n",
    "y_xgb_S = pipe_xgb_S.predict(X_test_S)\n",
    "y_xgb_L = pipe_xgb_L.predict(X_test_L)\n",
    "\n",
    "print(\"Results for data set S with XGBoost (Data Science Roles)\\n\", classification_report(y_test_S, y_xgb_S, target_names=lab_S.classes_, digits=4))\n",
    "print(\"\\nResults for data set L  With XGBoost (General career paths)\\n\", classification_report(y_test_L, y_xgb_L, target_names=lab_L.classes_, digits=4))\n",
    "\n",
    "\n",
    "proba_S = pipe_xgb_S.predict_proba(X_test_S)  # returns probabilities for S classes\n",
    "proba_L = pipe_xgb_L.predict_proba(X_test_L)  # returns probabilities for L classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 2105,
     "status": "ok",
     "timestamp": 1753261131832,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "50Z24EVdWfKW",
    "outputId": "7eb145cc-8d34-4ca0-abd8-c4c677efda47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline L saved to: C:\\Users\\Simon\\Documents\\GitHub\\RoleRecommender\\final_report\\notebooks\\downloads_for_streamlit\\pipe_xgb_L.pkl\n",
      "Pipeline S saved to: C:\\Users\\Simon\\Documents\\GitHub\\RoleRecommender\\final_report\\notebooks\\downloads_for_streamlit\\pipe_xgb_S.pkl\n"
     ]
    }
   ],
   "source": [
    "###---EXPORT FOR STREAMLIT---###\n",
    "# Get the current notebook directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Build the export paths\n",
    "export_path_L = os.path.join(current_dir, \"downloads_for_streamlit\", \"pipe_xgb_L.pkl\")\n",
    "export_path_S = os.path.join(current_dir, \"downloads_for_streamlit\", \"pipe_xgb_S.pkl\")\n",
    "\n",
    "# Save pipeline for y_L\n",
    "joblib.dump(pipe_xgb_L, export_path_L)\n",
    "print(f\"Pipeline L saved to: {export_path_L}\")\n",
    "\n",
    "# Save pipeline for y_S\n",
    "joblib.dump(pipe_xgb_S, export_path_S)\n",
    "print(f\"Pipeline S saved to: {export_path_S}\")\n",
    "###---EXPORT FOR STREAMLIT---###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 3047,
     "status": "ok",
     "timestamp": 1753261134898,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "2YVPFQ1WZ-LF",
    "outputId": "632c36e3-7258-4bfd-e467-f2aaa336ab49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report S saved to: C:\\Users\\Simon\\Documents\\GitHub\\RoleRecommender\\final_report\\notebooks\\downloads_for_streamlit\\classification_report_S.json\n",
      "Classification report L saved to: C:\\Users\\Simon\\Documents\\GitHub\\RoleRecommender\\final_report\\notebooks\\downloads_for_streamlit\\classification_report_L.json\n"
     ]
    }
   ],
   "source": [
    "###---EXPORT FOR STREAMLIT---###\n",
    "# Get the current notebook directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Build export paths\n",
    "export_path_S = os.path.join(current_dir, \"downloads_for_streamlit\", \"classification_report_S.json\")\n",
    "export_path_L = os.path.join(current_dir, \"downloads_for_streamlit\", \"classification_report_L.json\")\n",
    "\n",
    "# Generate Classification Reports\n",
    "report_S = classification_report(y_test_S, y_xgb_S, output_dict=True)\n",
    "report_L = classification_report(y_test_L, y_xgb_L, output_dict=True)\n",
    "\n",
    "# Save as JSON\n",
    "with open(export_path_S, \"w\") as f:\n",
    "    json.dump(report_S, f, indent=4)\n",
    "\n",
    "with open(export_path_L, \"w\") as f:\n",
    "    json.dump(report_L, f, indent=4)\n",
    "\n",
    "# Test output\n",
    "print(f\"Classification report S saved to: {export_path_S}\")\n",
    "print(f\"Classification report L saved to: {export_path_L}\")\n",
    "###---EXPORT FOR STREAMLIT---###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 1923,
     "status": "ok",
     "timestamp": 1753261136853,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "fTELP8o302ju",
    "outputId": "0afea5b5-1f19-4972-c23a-041dac5083c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix S saved to: C:\\Users\\Simon\\Documents\\GitHub\\RoleRecommender\\final_report\\notebooks\\downloads_for_streamlit\\confusion_matrix_S.json\n",
      "Confusion matrix L saved to: C:\\Users\\Simon\\Documents\\GitHub\\RoleRecommender\\final_report\\notebooks\\downloads_for_streamlit\\confusion_matrix_L.json\n"
     ]
    }
   ],
   "source": [
    "###---EXPORT FOR STREAMLIT---###\n",
    "# Get the current notebook directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Build export paths\n",
    "export_path_S = os.path.join(current_dir, \"downloads_for_streamlit\", \"confusion_matrix_S.json\")\n",
    "export_path_L = os.path.join(current_dir, \"downloads_for_streamlit\", \"confusion_matrix_L.json\")\n",
    "\n",
    "# --- Calculate Confusion Matrices ---\n",
    "cm_S = confusion_matrix(y_test_S, y_xgb_S).tolist()  # .tolist() converts NumPy array to nested list\n",
    "cm_L = confusion_matrix(y_test_L, y_xgb_L).tolist()\n",
    "\n",
    "# --- Optional: Label axes ---\n",
    "confusion_S = {\n",
    "    \"labels\": list(lab_S.classes_),\n",
    "    \"matrix\": cm_S\n",
    "}\n",
    "\n",
    "confusion_L = {\n",
    "    \"labels\": list(lab_L.classes_),\n",
    "    \"matrix\": cm_L\n",
    "}\n",
    "\n",
    "# --- Save as JSON files ---\n",
    "with open(export_path_S, \"w\") as f:\n",
    "    json.dump(confusion_S, f, indent=4)\n",
    "\n",
    "with open(export_path_L, \"w\") as f:\n",
    "    json.dump(confusion_L, f, indent=4)\n",
    "\n",
    "# Test output\n",
    "print(f\"Confusion matrix S saved to: {export_path_S}\")\n",
    "print(f\"Confusion matrix L saved to: {export_path_L}\")\n",
    "###---EXPORT FOR STREAMLIT---###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjzMifLITPT2"
   },
   "source": [
    "## Feature Importance with SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "d2Uxaornn1vk"
   },
   "outputs": [],
   "source": [
    "# This is code from Gemini, I admit. We use it for getting the correct feature names from the original DataFrame:\n",
    "\n",
    "def get_feature_names(column_transformer):\n",
    "    feature_names = []\n",
    "\n",
    "    # Iterate over each transformer in the ColumnTransformer\n",
    "    for name, transformer, columns in column_transformer.transformers_:\n",
    "\n",
    "        # Skip the 'remainder' placeholder for now\n",
    "        if name != 'remainder':\n",
    "\n",
    "            # If the transformer is a Pipeline, get the last step (usually the actual transformer)\n",
    "            if hasattr(transformer, 'named_steps'):\n",
    "                transformer = list(transformer.named_steps.values())[-1]\n",
    "\n",
    "            # Try to get feature names using the modern sklearn method\n",
    "            if hasattr(transformer, 'get_feature_names_out'):\n",
    "                names = transformer.get_feature_names_out(columns)\n",
    "\n",
    "            # Fall back to older sklearn versions' method if needed\n",
    "            elif hasattr(transformer, 'get_feature_names'):\n",
    "                names = transformer.get_feature_names(columns)\n",
    "\n",
    "            # If the transformer does not provide feature names, use the input column names/indices as is\n",
    "            else:\n",
    "                names = columns\n",
    "\n",
    "            # Add the extracted feature names to the final list\n",
    "            feature_names.extend(names)\n",
    "\n",
    "        # If this transformer is the 'remainder' part of the ColumnTransformer\n",
    "        else:\n",
    "            # If remainder is set to 'passthrough', add those columns as feature names directly\n",
    "            if transformer == 'passthrough':\n",
    "                feature_names.extend(columns)\n",
    "\n",
    "    # Return the full list of feature names after processing all transformers\n",
    "    return feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "rGk7PfRuP9FZ"
   },
   "outputs": [],
   "source": [
    "def shap_analysis_from_pipeline(pipe, X_train, filename_csv):\n",
    "    # Extract model and preprocessor from pipeline\n",
    "    model = pipe.named_steps['xgb']\n",
    "    preprocessor = pipe.named_steps['preprocessing']\n",
    "\n",
    "    # Sample and preprocess data\n",
    "    X_sample = X_train.sample(100, random_state=42)\n",
    "    X_transformed = preprocessor.transform(X_sample)\n",
    "\n",
    "    # Get feature names\n",
    "    feature_names = get_feature_names(preprocessor)\n",
    "\n",
    "    # Initialize SHAP explainer and calculate SHAP values\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_transformed)  # shape: (samples, features, classes)\n",
    "\n",
    "    # Get class names\n",
    "    class_names = model.classes_\n",
    "\n",
    "    # Rearrange SHAP values for class-wise iteration\n",
    "    shap_values_classwise = np.transpose(shap_values, (2, 0, 1))  # (classes, samples, features)\n",
    "\n",
    "    # Collect mean absolute SHAP values per class\n",
    "    shap_list = []\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        df_shap = pd.DataFrame(shap_values_classwise[i], columns=feature_names)\n",
    "        shap_mean = df_shap.abs().mean().sort_values(ascending=False)\n",
    "        shap_list.append(shap_mean.rename(f\"Mean |SHAP| ({class_name})\"))\n",
    "\n",
    "    # Combine results into one DataFrame\n",
    "    df_shap_all = pd.concat(shap_list, axis=1)\n",
    "\n",
    "    # Display the combined table\n",
    "    display(df_shap_all)\n",
    "\n",
    "    # Save to CSV\n",
    "    df_shap_all.to_csv(filename_csv, sep = ';') # <-- Dies ist dein ursprünglicher Speichercode\n",
    "\n",
    "    # Plot SHAP summary bar chart\n",
    "   # shap.summary_plot(shap_values, X_transformed, feature_names=feature_names, plot_type=\"bar\")\n",
    "\n",
    "    return df_shap_all # Saves the DataFrame for later download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 830
    },
    "executionInfo": {
     "elapsed": 2754,
     "status": "ok",
     "timestamp": 1753261139641,
     "user": {
      "displayName": "Jan Simon",
      "userId": "01142490666305147069"
     },
     "user_tz": -120
    },
    "id": "y3xBHohriNe8",
    "outputId": "596d940a-7593-4cc1-975b-572029a3d903"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean |SHAP| (0)</th>\n",
       "      <th>Mean |SHAP| (1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Q6_For how many years have you been writing code and/or programming?</th>\n",
       "      <td>0.317964</td>\n",
       "      <td>0.317964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q4_What is the highest level of formal education that you have attained or plan to attain within the next 2 years?</th>\n",
       "      <td>0.246095</td>\n",
       "      <td>0.246095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q7_No. of Regular used programming languages?</th>\n",
       "      <td>0.222353</td>\n",
       "      <td>0.222353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q15_For how many years have you used machine learning methods?</th>\n",
       "      <td>0.207601</td>\n",
       "      <td>0.207601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q14_No. of DataViz Libs or Tools?</th>\n",
       "      <td>0.139828</td>\n",
       "      <td>0.139828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q32_Which of the following business intelligence tools do you use most often?_SAP Analytics Cloud</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q32_Which of the following business intelligence tools do you use most often?_Qlik</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q32_Which of the following business intelligence tools do you use most often?_Other</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q32_Which of the following business intelligence tools do you use most often?_Looker</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q32_Which of the following business intelligence tools do you use most often?_Domo</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Mean |SHAP| (0)  \\\n",
       "Q6_For how many years have you been writing cod...         0.317964   \n",
       "Q4_What is the highest level of formal educatio...         0.246095   \n",
       "Q7_No. of Regular used programming languages?              0.222353   \n",
       "Q15_For how many years have you used machine le...         0.207601   \n",
       "Q14_No. of DataViz Libs or Tools?                          0.139828   \n",
       "...                                                             ...   \n",
       "Q32_Which of the following business intelligenc...         0.000000   \n",
       "Q32_Which of the following business intelligenc...         0.000000   \n",
       "Q32_Which of the following business intelligenc...         0.000000   \n",
       "Q32_Which of the following business intelligenc...         0.000000   \n",
       "Q32_Which of the following business intelligenc...         0.000000   \n",
       "\n",
       "                                                    Mean |SHAP| (1)  \n",
       "Q6_For how many years have you been writing cod...         0.317964  \n",
       "Q4_What is the highest level of formal educatio...         0.246095  \n",
       "Q7_No. of Regular used programming languages?              0.222353  \n",
       "Q15_For how many years have you used machine le...         0.207601  \n",
       "Q14_No. of DataViz Libs or Tools?                          0.139828  \n",
       "...                                                             ...  \n",
       "Q32_Which of the following business intelligenc...         0.000000  \n",
       "Q32_Which of the following business intelligenc...         0.000000  \n",
       "Q32_Which of the following business intelligenc...         0.000000  \n",
       "Q32_Which of the following business intelligenc...         0.000000  \n",
       "Q32_Which of the following business intelligenc...         0.000000  \n",
       "\n",
       "[72 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean |SHAP| (0)</th>\n",
       "      <th>Mean |SHAP| (1)</th>\n",
       "      <th>Mean |SHAP| (2)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Q6_For how many years have you been writing code and/or programming?</th>\n",
       "      <td>0.319807</td>\n",
       "      <td>0.145118</td>\n",
       "      <td>0.449246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q31_A No. of BI Tools used?</th>\n",
       "      <td>0.196799</td>\n",
       "      <td>0.006945</td>\n",
       "      <td>0.247064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q18_No. of Computer Vsion methods?</th>\n",
       "      <td>0.173698</td>\n",
       "      <td>0.037402</td>\n",
       "      <td>0.138553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q27_A No. of Current Cloud Products?</th>\n",
       "      <td>0.145569</td>\n",
       "      <td>0.028404</td>\n",
       "      <td>0.041869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q37_No. of Learning Platforms?</th>\n",
       "      <td>0.120146</td>\n",
       "      <td>0.040045</td>\n",
       "      <td>0.026904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q32_Which of the following business intelligence tools do you use most often?_Domo</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q32_Which of the following business intelligence tools do you use most often?_Looker</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q32_Which of the following business intelligence tools do you use most often?_Microsoft Power BI</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q32_Which of the following business intelligence tools do you use most often?_Other</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q32_Which of the following business intelligence tools do you use most often?_Amazon QuickSight</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Mean |SHAP| (0)  \\\n",
       "Q6_For how many years have you been writing cod...         0.319807   \n",
       "Q31_A No. of BI Tools used?                                0.196799   \n",
       "Q18_No. of Computer Vsion methods?                         0.173698   \n",
       "Q27_A No. of Current Cloud Products?                       0.145569   \n",
       "Q37_No. of Learning Platforms?                             0.120146   \n",
       "...                                                             ...   \n",
       "Q32_Which of the following business intelligenc...         0.000000   \n",
       "Q32_Which of the following business intelligenc...         0.000000   \n",
       "Q32_Which of the following business intelligenc...         0.000000   \n",
       "Q32_Which of the following business intelligenc...         0.000000   \n",
       "Q32_Which of the following business intelligenc...         0.000000   \n",
       "\n",
       "                                                    Mean |SHAP| (1)  \\\n",
       "Q6_For how many years have you been writing cod...         0.145118   \n",
       "Q31_A No. of BI Tools used?                                0.006945   \n",
       "Q18_No. of Computer Vsion methods?                         0.037402   \n",
       "Q27_A No. of Current Cloud Products?                       0.028404   \n",
       "Q37_No. of Learning Platforms?                             0.040045   \n",
       "...                                                             ...   \n",
       "Q32_Which of the following business intelligenc...         0.000000   \n",
       "Q32_Which of the following business intelligenc...         0.000000   \n",
       "Q32_Which of the following business intelligenc...         0.000000   \n",
       "Q32_Which of the following business intelligenc...         0.000000   \n",
       "Q32_Which of the following business intelligenc...         0.000000   \n",
       "\n",
       "                                                    Mean |SHAP| (2)  \n",
       "Q6_For how many years have you been writing cod...         0.449246  \n",
       "Q31_A No. of BI Tools used?                                0.247064  \n",
       "Q18_No. of Computer Vsion methods?                         0.138553  \n",
       "Q27_A No. of Current Cloud Products?                       0.041869  \n",
       "Q37_No. of Learning Platforms?                             0.026904  \n",
       "...                                                             ...  \n",
       "Q32_Which of the following business intelligenc...         0.000000  \n",
       "Q32_Which of the following business intelligenc...         0.000000  \n",
       "Q32_Which of the following business intelligenc...         0.000000  \n",
       "Q32_Which of the following business intelligenc...         0.000000  \n",
       "Q32_Which of the following business intelligenc...         0.000000  \n",
       "\n",
       "[71 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP feature importance L saved to: C:\\Users\\Simon\\Documents\\GitHub\\RoleRecommender\\final_report\\notebooks\\downloads_for_streamlit\\shap_feature_importance_all_classes_L.csv\n",
      "SHAP feature importance S saved to: C:\\Users\\Simon\\Documents\\GitHub\\RoleRecommender\\final_report\\notebooks\\downloads_for_streamlit\\shap_feature_importance_all_classes_S.csv\n"
     ]
    }
   ],
   "source": [
    "# Call for model L, and capture the returned DataFrame\n",
    "df_shap_all_L = shap_analysis_from_pipeline(\n",
    "    pipe_xgb_L, X_train_L,\n",
    "    os.path.join(os.getcwd(), \"downloads_for_streamlit\", \"shap_feature_importance_all_classes_L.csv\")\n",
    ")\n",
    "\n",
    "# Call for model S, and capture the returned DataFrame\n",
    "df_shap_all_S = shap_analysis_from_pipeline(\n",
    "    pipe_xgb_S, X_train_S,\n",
    "    os.path.join(os.getcwd(), \"downloads_for_streamlit\", \"shap_feature_importance_all_classes_S.csv\")\n",
    ")\n",
    "\n",
    "###---EXPORT FOR STREAMLIT---###\n",
    "# Get the current notebook directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Build export paths\n",
    "export_path_L = os.path.join(current_dir, \"downloads_for_streamlit\", \"shap_feature_importance_all_classes_L.csv\")\n",
    "export_path_S = os.path.join(current_dir, \"downloads_for_streamlit\", \"shap_feature_importance_all_classes_S.csv\")\n",
    "\n",
    "# Test output\n",
    "print(f\"SHAP feature importance L saved to: {export_path_L}\")\n",
    "print(f\"SHAP feature importance S saved to: {export_path_S}\")\n",
    "###---EXPORT FOR STREAMLIT---###"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1aXgwAsFPNCo-I_3AmfHDDl-mbMFuqmDf",
     "timestamp": 1749404422049
    },
    {
     "file_id": "1JCjLAqw1GA0s2f1A-BUPI-7Cp9wUZXd6",
     "timestamp": 1748885543791
    },
    {
     "file_id": "1jmdoFNwP29X0PTg4OqT3DPkIkBXkP9jl",
     "timestamp": 1748094260592
    },
    {
     "file_id": "1bw1oLViHjd-k2wztUnu8xRvlA5hPsov_",
     "timestamp": 1747054697519
    },
    {
     "file_id": "1N_iIIcytq2esfepkk9CVbbWpZYKTzE5a",
     "timestamp": 1743324642691
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
